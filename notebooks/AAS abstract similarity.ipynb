{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: this has been superseded by the blog post in adrn.github.io-source/content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard lib\n",
    "import re\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party \n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.utils.extmath import cartesian\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import yaml\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to configuration file with login information to the AAS SQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_filename = \"/Users/adrian/projects/aas-abstract-sorter/sql_login.yml\"\n",
    "with open(config_filename) as f:\n",
    "    config = yaml.load(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "engine = create_engine('mysql+pymysql://{user}:{password}@{server}/{database}'.format(**config))\n",
    "engine.connect()\n",
    "_presentation_cache = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all presentations and sessions from AAS 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT session.so_id, presentation.title, \n",
    "  presentation.abstract, presentation.id\n",
    "FROM session, presentation\n",
    "WHERE session.meeting_code = 'aas227'\n",
    "  AND session.so_id = presentation.session_so_id\n",
    "  AND presentation.status IN ('Sessioned', '')\n",
    "  AND session.type IN (\n",
    "      'Oral Session'\n",
    "    , 'Special Session'\n",
    "    , 'Splinter Meeting'\n",
    "    )\n",
    "ORDER BY presentation.id;\n",
    "\"\"\"\n",
    "result = engine.execute(query)\n",
    "all_results = result.fetchall()\n",
    "presentation_df = pd.DataFrame(all_results, columns=all_results[0].keys())\n",
    "presentation_df['abstract'] = presentation_df['abstract'].str.replace('<[^<]+?>', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT session.title, session.start_date_time, session.end_date_time, session.so_id\n",
    "FROM session\n",
    "WHERE session.meeting_code = 'aas227'\n",
    "  AND session.type IN (\n",
    "      'Oral Session'\n",
    "    , 'Special Session'\n",
    "    , 'Splinter Meeting'\n",
    "    )\n",
    "ORDER BY session.so_id;\n",
    "\"\"\"\n",
    "result = engine.execute(query)\n",
    "session_results = result.fetchall()\n",
    "session_df = pd.DataFrame(session_results, columns=session_results[0].keys())\n",
    "session_df['start_date_time'] = pd.to_datetime(session_df['start_date_time'])\n",
    "session_df['end_date_time'] = pd.to_datetime(session_df['end_date_time'])\n",
    "session_df = session_df[1:] # zero-th entry has a corrupt date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a scikit-learn count vectorizer with a custom word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# based on http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remove non letters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # stem\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "vectorizer = text.CountVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=tokenize,\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the count vectorizer to all AAS abstracts from AAS 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 5568)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix = vectorizer.fit_transform(presentation_df['abstract']).toarray()\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a quick check, what are the 10 most common words in AAS abstracts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['galaxi' 'star' 'thi' 'observ' 'mass' 'use' 'model' 'survey' 'format'\n",
      " 'present']\n"
     ]
    }
   ],
   "source": [
    "ten_most_common_idx = count_matrix.sum(axis=0).argsort()[::-1][:10]\n",
    "feature_words = np.array(vectorizer.get_feature_names())\n",
    "print(feature_words[ten_most_common_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each pair of abstracts, compute the cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similiarity_matrix = np.zeros((count_matrix.shape[0],count_matrix.shape[0]))\n",
    "for ix1 in range(count_matrix.shape[0]):\n",
    "    for ix2 in range(count_matrix.shape[0]):\n",
    "        num = count_matrix[ix1].dot(count_matrix[ix2]) \n",
    "        denom = np.linalg.norm(count_matrix[ix1]) * np.linalg.norm(count_matrix[ix2])\n",
    "        \n",
    "        if num < 1: # if no common words, the vectors are orthogonal\n",
    "            v = 0.\n",
    "        else:\n",
    "            v = num / denom\n",
    "            \n",
    "        similiarity_matrix[ix1,ix2] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the top ten most similar abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similiarity_matrix_1d = np.triu(similiarity_matrix).ravel()\n",
    "top_ten = sorted(np.unique(similiarity_matrix_1d[~np.isclose(similiarity_matrix_1d,1.)]), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constraining the atmosphere of exoplanet WASP-34b\n",
      "Analysis of Secondary Eclipse Observations of Hot-Jupiters WASP-26b and CoRoT-1b\n",
      "\n",
      "Constraining the atmosphere of exoplanet WASP-34b\n",
      "Atmospheric, Orbital and Secondary Eclipse Analysis of HAT-P-30-WASP-51b\n",
      "\n",
      "Constraining the atmosphere of exoplanet WASP-34b\n",
      "Secondary Eclipse Observations and Orbital Analysis of WASP-32b\n",
      "\n",
      "Analysis of Secondary Eclipse Observations of Hot-Jupiters WASP-26b and CoRoT-1b\n",
      "Atmospheric, Orbital and Secondary Eclipse Analysis of HAT-P-30-WASP-51b\n",
      "\n",
      "Analysis of Secondary Eclipse Observations of Hot-Jupiters WASP-26b and CoRoT-1b\n",
      "Secondary Eclipse Observations and Orbital Analysis of WASP-32b\n",
      "\n",
      "Atmospheric, Orbital and Secondary Eclipse Analysis of HAT-P-30-WASP-51b\n",
      "Secondary Eclipse Observations and Orbital Analysis of WASP-32b\n",
      "\n",
      "How Giant Planets Shape the Characteristics of Terrestrial Planets\n",
      "The Fragility of the Terrestrial Planets During a Giant Planet Instability\n",
      "\n",
      "Galaxy Structure as a Driver of the Star Formation Sequence Slope and Scatter\n",
      "Star formation histories of z~2 galaxies and their intrinsic characteristics on the SFR-M* plane\n",
      "\n",
      "The Legacy of NASA Astrophysics E/PO: Conducting Professional Development, Developing Key Themes & Resources, and Broadening E/PO Audiences\n",
      "The Legacy of NASA Astrophysics E/PO: Scientist Engagement and Higher Education\n",
      "\n",
      "The Undergraduate ALFALFA Team: Collaborative Research Projects\n",
      "The Undergraduate ALFALFA Team: A Model for Involving Undergraduates in Large Astronomy Collaborations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ix1,ix2 in zip(list(ix[0]), list(ix[1])):\n",
    "    pres1 = get_presentation(presentation_ids[ix1])\n",
    "    pres2 = get_presentation(presentation_ids[ix2])\n",
    "    print(pres1['title'])\n",
    "    print(pres2['title'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those seem pretty similar! Looks like the code is working..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll predict which simultaneous sessions have the most overlap\n",
    "\n",
    "For now, we'll start with the first day of conference talks, 5 Jan. We'll also only check for sessions that have the same start time (of course, we should really be looking at any overlapping sessions, but this is fine as a first pass...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def session_similarity(so_id1, so_id2):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two sessions by getting the sub-matrix of the \n",
    "    similarity matrix for all pairs of presentations from each session.\n",
    "    \"\"\"\n",
    "    presentations_session1 = presentation_df[presentation_df['so_id'] == so_id1]\n",
    "    presentations_session2 = presentation_df[presentation_df['so_id'] == so_id2]\n",
    "    \n",
    "    if len(presentations_session1) == 0 or len(presentations_session2) == 0:\n",
    "        # no presentations in session\n",
    "        return np.array([])\n",
    "    \n",
    "    index_pairs = cartesian((presentations_session1.index,presentations_session2.index)).T\n",
    "    sub_matrix = similiarity_matrix[(index_pairs[0],index_pairs[1])]\n",
    "    \n",
    "    shape = (len(presentations_session1), len(presentations_session2))\n",
    "    sub_matrix = sub_matrix.reshape(shape)\n",
    "    \n",
    "    return sub_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intergalactic Medium, QSO Absorption Line Systems\n",
      "Gas and Dust Content in Distant Galaxies\n",
      "0.570158254448 0.223779436451\n",
      "\n",
      "SDSS-IV MaNGA: Mapping Nearby Galaxies at Apache Point Observatory\n",
      "The REsolved Spectroscopy Of a Local VolumE (RESOLVE) Survey and its Environmental COntext (ECO)\n",
      "0.522251721857 0.245616652748\n",
      "\n",
      "Extrasolar Planets: Hosts, Interactions, Formation, and Interiors\n",
      "Formation and Evolution of Stars and Stellar Systems\n",
      "0.500320769881 0.103147252391\n",
      "\n",
      "Physical Properties of High Redshift Galaxies\n",
      "Structure and Physics of Galaxies at z<~0.2\n",
      "0.596678120671 0.300710183535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name,group in session_df[session_df['start_date_time'] >= datetime(2016, 1, 5)].groupby('start_date_time'):\n",
    "    for title1,so_id1 in zip(group['title'],group['so_id']):\n",
    "        for title2,so_id2 in zip(group['title'],group['so_id']):\n",
    "            if so_id1 >= so_id2: continue\n",
    "                \n",
    "            scores = session_similarity(so_id1, so_id2)\n",
    "            \n",
    "            if len(scores) == 0: # no presentations in one of the sessions\n",
    "                continue \n",
    "            \n",
    "            if scores.max() > 0.5: # totally arbitrary threshold\n",
    "                print(title1)\n",
    "                print(title2)\n",
    "                print(scores.max(), np.median(scores))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are sessions that were scheduled for the same time-slot that have two talks with significant overlap between their abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": false
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
